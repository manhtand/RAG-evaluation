{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine, SubQuestionQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "df = pd.read_csv(\"questions/Lyft2021_queries.csv\")\n",
    "queries = df[\"Query\"].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_vector_index = VectorStoreIndex(nodes)\n",
    "vector_retriever = top_vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
    "    recursive_retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=retriever_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"recursive_retriever\",\n",
    "            description=\"Recursive retriever for accessing documents\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qa_dataset = []\n",
    "\n",
    "for query in queries:\n",
    "    retrieved_docs = query_engine.query(query).source_nodes\n",
    "    qa_dataset.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_context\": [doc.node.node_id for doc in retrieved_docs]\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import ContextRelevancyEvaluator\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "evaluator = ContextRelevancyEvaluator(llm=llm)\n",
    "results = []\n",
    "\n",
    "for qa_data in qa_dataset:\n",
    "    result = evaluator.aevaluate(qa_data[\"query\"], contexts=qa_data[\"retrieved_context\"])\n",
    "    results.append(result)\n",
    "\n",
    "results = await tqdm_asyncio.gather(*results)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "evaluator = RetrieverEvaluator.from_metric_names(metrics, retriever=recursive_retriever)\n",
    "\n",
    "eval_results = []\n",
    "for entry in qa_dataset:\n",
    "    query = entry[\"query\"]\n",
    "    context = entry[\"retrieved_context\"]\n",
    "\n",
    "    result = evaluator.evaluate(\n",
    "        query=query,\n",
    "        expected_ids=context\n",
    "    )\n",
    "    eval_results.append(result)\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_results(\"recursive retrieval\",  eval_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
